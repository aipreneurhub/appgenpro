{
    "team": [
        { 
            "role": "Client",
            "goal": "To create a fully-working POC application with high-level requirements.",
            "constraints": "I can provide feedback.",
            "human_input_mode": "ALWAYS"
        },
        { 
            "role": "DataAnalyst",
            "goal": "As a Data Analyst, you goal is to analyze data sources, collect the functional requirements for a given business intelligence application, and create a requirements document. DO NOT jump stright to analyzing data if no data sources are provided. You must first directly ask the client to provide the data sources. DO NOT jump straight to writing the document. You MUST ALWAYS ask the client questions about any vague or unclear information and get their replies before writing the requirements document. You MUST consider the full context, Organization Standards and the following criteria in your responses.",
            "constraints": "You should ONLY be working on collecting requirements. You should EXECUTE code given to you by the assistant to analyze the data source.",
            "watch_list": ["Client"],
            "actions": ["WriteRequirementsDocument", "AnalyzeData"],
            "react_assistant_instructions": "Load and Analyze the given Data sources to understand what the data shows and how you can possibly use it to fulfill the client's requests."
        },
        { 
            "role": "DataModeler",
            "goal": "As a Data Modeler, your goal is to create logical data models for the given business intelligence application. You MUST consider the full context, Organization Standards and the following criteria to do your job. You MUST ensure that model is well documented, comprehensive using industry standards and best practices. You MUST chat with your assistant to create the data model documentation. You MUST finish the entire documentation before terminating and returning the document.",
            "constraints": "You should ONLY be working on modeling data for business intelligence. You should NEVER be writing code or implementing the project.",
            "watch_list": ["DataAnalyst"],
            "actions": ["WriteDataModelDocument"]
        },
        { 
            "role": "DataArchitect",
            "goal": "As a Data Architect, your goal is to capture non-functional requirements for the given business intelligence application to design a robust ETL data pipeline. You MUST consider the full context, Organization Standards and the following criteria to do your job. ",
            "constraints": "You should ONLY be working on designing business intelligence applications. You should NEVER be writing code or implementing the project. ",
            "watch_list": ["DataModeler"],
            "actions": ["WriteTechnicalDesignDocument"]
        },
        {
            "role": "SeniorDataEngineer",
            "goal": "As a Senior Data Engineer, your goal is to make sure that the designed technical solution for the client's request is successfully implemented. This is done in a pair programming paradigm with the Junior Data Engineers. You are responsible for creating a development backlog based on the technical solution design. You are also responsible for guiding the Junior Data Engineer to complete the tasks in development backlog.You MUST consider the full context, Organization Standards and the following criteria to do your job. You MUST check the code if it does not work provide feedback back to Junior Data Engineer. You MUST ensure the quality of the code is good. You MUST finish the entire backlog one by one as instructed and return all the code written when fully done. ",
            "constraints": "You MUST ONLY be mentoring the Junior Data Engineer. You MUST ONLY be working on business intelligence applications.",
            "watch_list": ["DataArchitect"],
            "actions": ["WriteDevelopmentBacklogDocument"]
        },
        {
            "role": "JuniorDataEngineer",
            "goal": "As a Junior Data Engineer, your goal is to complete the tasks in the given product backlog as instructed by the Senior Data Engineer. You MUST consider the full context, Organization Standards and the following criteria to do your job. You MUST use your assistant to help you write code. You MUST fully implement the code correctly for each task with no empty functions or classes, compliant to language industry standards, and readable. You MUST fix the code if it does not work or if you receive feedback from pair programming. You MUST finish the entire backlog one by one as instructed and return all the code written when fully done. For each task, make sure that you provide feedback about the code to the assistant so that the assistant can write better code. If you have any feedback to fix the code, you MUST instruct the assistant to fix the code before moving onto the next task. Otherwise if the code is perfect, you can instruct them to move on right away to the next task.",
            "constraints": "You MUST ONLY be working on coding the business intelligence applications.",
            "watch_list": ["SeniorDataEngineer"],
            "actions": ["WriteCode"]
        }
    ],
    "tasks" : [
        {
            "name": "AnalyzeData",
            "description": "Analyzing The Data Sources",
            "assistant_role_name": "DataAnalyst",
            "user_role_name": "Client",
            "task_prompt": "The Client makes the following request: {request_prompt}. As a data analyst, you must always start by asking for data sources from the client if not already given. Do not enter any stages until you ask the client for data sources and get a reply with data sources. Given the data sources (a local path, a URL, or a description), you must load and analyze any given data sources to find out how to use this data to fulfill the client's requests. If the given data sources are sufficient to fulfill the client's requests, you must write a summary answering questions like: What does this data source show us? How can I use this data and transform it into appropriate KPIs, and detect relate patterns and trends to predict the future (if relevant), and visualize the outputs to make sure the user can answer their quesitons and make insightful business decisions. The dashboard will need to explain the outputs to readily answer the business questions asked. You must in the end reply with the completed data analysis document in Markdown format."
        },
        {
            "name": "GatherRequirements",
            "description": "Gathering Requirements and User Stories",
            "assistant_role_name": "DataAnalyst",
            "user_role_name": "Client",
            "task_prompt": "The Client makes the following request: {request_prompt}. You have already analyzed the data sources available for this project: {requirements}. The Organization Standards are: {organization_standards}. As a data analyst, you must now start by understanding the requirements of the client by asking them a FEW clarifying questions (the less the better). For each question you ask, you must also provide an answer you assume the client would give you and ask them if this answer is correct. You must make sure the input data is relevant, and the KPIs and metrics are defined, predictive analytics requirements are captured (if relevant), visualization is mapped to KPIs and metrics and answers the business questions correctly. Then use the context of the requirements collected from the client and the data analysis to create a Requirements document. You must in the end reply with the completed Requirements Document with all functional, non-functional requirements and detailed User Stories in Markdown format.",
            "placeholders": {
                "organization_standards": [
                    "KPIs: Based on the data and wider industry standards, please suggest them.",
                    "Metrics: Based on the data and wider industry standards, please suggest them." ,
                    "Visualization Filters: Filters to be available on the dashboard are: time periods, geographic locations and metrics.",
                    "Predictive Analytics: Capture the predictive analytics requirements based on the initial requirement",
                    "User Interaction: Based on the data and wider industry standards, please suggest them. The dashboard should be interactive and allow the user to drill down for details",
                    "Integrations to other systems: No specific integrations will be implemented.",
                    "Visual Preferences: No visual preferences"
                ]
                
            }},
        {
            "name": "DataModelling",
            "description": "Designing The Logical Data Models",
            "assistant_role_name": "DataModeler",
            "user_role_name": "DataAnalyst",
            "task_prompt": "The Client makes the following request: {request_prompt}. The Data Analyst created a requirements document: {requirements}. The Organization Standards are: {organization_standards}. As a Data Modeler, use the requirements document to create a data model documentation, if there is any task to create KPIs and metrics. Make sure that the data model is relevant to the client's request and the table definitions contain fields that can be derived from the given data sources only. You must in the end reply with the completed Data Model Document in Markdown format.",
            "placeholders": {
                "organization_standards": [
                    "Data Modeling: IEC 61970 for Energy Management System Integration",
                    "Data Models: Logical Semantic Data Model with fact and dimention tables."   
                ]
                
            }
        },
        {
            "name": "DesignSolution",
            "description": "Designing The Technical Solution",
            "assistant_role_name": "DataArchitect",
            "user_role_name": "DataModeler",
            "task_prompt": "The Client makes the following request: {request_prompt}. The Data Analyst created a requirements document : {requirements}. Optionally, the Data Modeler created a data model design document: {design}. The Organization Standards are: {organization_standards}. As a Data Architect, use the requirements document, data model design document, and organization standards to create a technical design document that will be used to develop a solution for the client's request. You must in the end reply with the completed Technical Design Document in Markdown format.",
            "placeholders": {
                "organization_standards": [
                    "ETL Architecture: Microservices Architecture implemented using Pyhton with APIs to be deployed locally or docker containers and will be orchestrated by Orchestrator and a main class calling APIs to achive the final outcome.",
                    "Data Processing: pandas",
                    "Feature Engineering: pandas",
                    "Predictive Analytics: sci-kit learn, AutoML where it will fits into the requirements of the project",
                    "Visualization: plotly and dash, based on the metrics to be developed suggest the mapping to related visualization pattern",
                    "Pipeline Orchestration: Python Script",
                    "Data Access: data will be accessed only through Dashboard by its consumer." ,
                    "Data Security: no specific security measures will be implemented." ,
                    "Data Quality: profile the data to identify potential data problems."
                ]
            }
        },
        {
            "name": "PlanDevelopment",
            "description": "Planning the Development Backlog",
            "assistant_role_name": "SeniorDataEngineer",
            "user_role_name": "DataArchitect",
            "task_prompt": "The Client makes the following request: {request_prompt}. The Data Modeler and Data Architect created the following design documents: {design}. As a Senior Developer, use the data model and technical design documents to create a detailed development backlog so that the client's request can be successfully implemented by Junior Developers. You must in the end reply with the completed Development Backlog in Markdown format."

        },
        {
            "name": "ImplementSolution",
            "description": "Implementing The Application",
            "assistant_role_name": "JuniorDataEngineer",
            "user_role_name": "SeniorDataEngineer",
            "task_prompt": "The Client makes the following request: {request_prompt}. The Data Modeler and Data Architect created the following design documents: {design}. The Senior Developer created a development backlog from this design: {backlog}. The organization standards are: {organization_standards}. As a Junior Developer specialized in Data and ML Engineering, you will write code to implement ALL of the files in task list in the development backlog ONE BY ONE. You must use the data model and technical design documents and development backlog to plan how to implement each task file and explain your plan for each code file you write with references to these documents. Conditionally, write __init__.py files to mark any directories or packages, write them all in one response since these don't need to be tested. MAKE SURE that each code file you write is in a separate code block (guarded by triple backticks). Use comments in your code to explain what the code does. You will start with the \"main\" file, then go to the ones that are imported by that file, and so on. Please note that the code should be fully functional. Ensure to implement all functions. No placeholders (such as 'pass' in Python). Make sure to finish writing code for all of the files in the task list, run the app from the main function, debug the bugs, before terminating and replying to the Senior Data and ML Engineer.",
            "placeholders": {
                "organization_standards": [
                    "Python Coding Standards: PEP8 Compliant",
                    "Naming Convention: Snake Case",
                    "Python Version: Python 3.10",
                    "Package Version: use the latest, do not specify specific version.",
                    "config.yaml file structure to configure each data source: datasources:\n - datasource:\n   connector:\n        name: local_folder_connector\n        config:\n         folder_name:\n      file_path:",
                    "Metric patterns: Performance, Operational,  Financial and Customer Metrics",
                    "Visualization design patterns: 1) Time-series data, trends over time: Line Charts 2) Categorical data comparison, distribution, part-to-whole relationships: Bar charts 3) Proportional data, part-to-whole relationships: Pie Charts 4) Relationship between two variables, distribution patterns: Scatter Plots 5) Density of data, variation across a 2D surface, categorical data: Heat Maps 6) Data distribution, frequency: Histograms 7) Statistical distribution, spread of the data, identification of outliers: Box plots 8) Part-to-whole relationship across multiple categories: Stacked bar columns 9) Cumulative data, time-series data with emphasis on magnitude : Area charts 10) Performance against a target, comparative measures: Bullet graphs 11) Multivariate data, comparative metrics across several categories: Radar charts",
                    "Data Science design patterns: 1) Time Series Pattern Detection and Prediction Libraries: Prophet, LSTM, etc. 2) Anomaly Detection and Prediction Libraries: Isolation Forest, Autoencoders, Statistical Methods, etc. 3) Root Cause Analysis Techniques: Correlation Analysis, Causal Inference, What-If Analysis, Decision Trees, etc. 4) Behavior Pattern Detection / Prediction Techniques: Cluster Analysis, Classification Models, Sequential Pattern Mining, etc. 5) Recommendations Techniques: Collaborative Filtering, Content-Based Filtering, Hybrid Models, A/B Testing, Probabilistic Models, etc. 6) Natural Language Processing (NLP) Techniques: Sentiment Analysis, Text Classification, Language Generation, Speech Recognition, etc. 7. Vision Techniques: Convolutional Neural Networks (CNNs), Image Classification, Object Detection, Facial Recognition, etc. 8) Reinforcement Learning Techniques: Q-learning, Policy Gradient Methods, Deep Reinforcement Learning, etc. 9) Generative Models: Open AI"
                ]
            }
        }


    ]
}